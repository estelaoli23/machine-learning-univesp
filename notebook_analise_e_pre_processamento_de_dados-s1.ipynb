{"cells":[{"cell_type":"markdown","metadata":{"id":"V8kC3Anvq93I"},"source":["Este notebook é uma adaptação para uso no ambiente Google Colab do notebook **notebook_01.ipynb** fornecido como material complementar do livro Inteligência Artificial: Uma Abordagem de Aprendizado de Máquina|FACELI, Katti; LORENA, Ana C.; GAMA, João; AL, et. Tendo sido desenvolvido originalmente por: Renato Moraes Silva."]},{"cell_type":"markdown","metadata":{"id":"-owHGMOtr96U"},"source":["Antes de iniciar este notebook, salve o arquivo do conjunto de dados iris (irs.csv) em um diretório local na sua máquina"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnnnOe6sX8Jx"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mA execução de células com 'Python 3.11.2' requer o pacote ipykernel.\n","\u001b[1;31mExecute o seguinte comando para instalar \"ipykernel\" no ambiente do Python. \n","\u001b[1;31mComando: \"c:/Users/Pichau/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall\""]}],"source":["# O código abaixo lê o arquivo csv\n","\n","import pandas as pd\n","\n","files = pd.read_csv('./iris.csv')\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"]},{"cell_type":"markdown","metadata":{"id":"uwBc2SgHFx6k"},"source":["## Introdução\n","\n","Frequentemente, a visualização dos dados auxilia na interpretação e na análise de como eles estão distribuídos. O Python possui algumas bibliotecas que facilitam o processo de visualização, tais como: `Pandas`, `Matplotlib` e `Seaborn`. Para aprender a usar essas ferramentas, será usada a base de dados Iris. É importante destacar que a base de dados Iris usada neste exercício foi modificada pelos autores por motivos didáticos. A versão original dela pode ser encontrada no seguinte link: <https://archive.ics.uci.edu/ml/datasets/iris>. Usando a versão modificada dessa base de dados, será abordado como fazer a eliminação de atributos irrelevantes e o tratamento de valores faltantes. Também será mostrado como tratar valores redundantes ou inconsistentes e como fazer a normalização dos dados. Depois, será feita a detecção e remoção de *outliers* da base dados. Por fim, será mostrado como fazer a análise da distribuição das classes e da correlação entre os atributos. Ao final deste *notebook*, espera-se que o leitor tenha assimilado as principais etapas necessárias para fazer a análise dos dados e prepará-los para serem usados pelos algoritmos de aprendizado de máquina.\n","\n","Este *notebook* se refere aos Capítulos **2 (Análise de Dados)** e **3 (Pré-processamento de Dados)** do livro Inteligência Artificial: Uma Abordagem de Aprendizado de Máquina."]},{"cell_type":"markdown","metadata":{"id":"JQMuULxSFx6l"},"source":["---\n","## Carregando os dados\n","\n","Primeiro, vamos importar todas as bibliotecas que serão usadas ao longo deste exercício."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3GBfdtgtFx6m"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\n","import numpy as np # importa a biblioteca usada para trabalhar com vetores e matrizes\n","import pandas as pd # importa a biblioteca usada para trabalhar com dataframes (dados em formato de tabela) e análise de dados\n","\n","# bibliotecas usadas para geracao de graficos\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","print('Bibliotecas carregadas com sucesso')"]},{"cell_type":"markdown","metadata":{"id":"cgB5HnhOFx6o"},"source":["Em seguida, os dados do conjunto de dados iris serão importados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AARSxAhVYd6Y"},"outputs":[],"source":["#importa o arquivo e guarda em um dataframe do Pandas\n","df_dataset = pd.read_csv( 'iris.csv', sep=',', index_col=None) \n","\n","print('Dados importados com sucesso!')"]},{"cell_type":"markdown","metadata":{"id":"eBmpwz6RRTea"},"source":["Agora, vamos dar uma olhada nas 10 primeiras amostras da base de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJ5qgUqDR4SV"},"outputs":[],"source":["# exibe os 10 primeiros elementos do dataframe\n","display(df_dataset.head(n=10))"]},{"cell_type":"markdown","metadata":{"id":"DAjEfL1EFx6q"},"source":["  A base de dados contém amostras de flores (linhas) representadas pelos seguintes atributos (colunas): `id_planta`, `comprimento_sepala`, `largura_sepala`, `comprimento_petala`, `largura_petala` e `cidade_origem`. Por fim, temos o atributo `classe` que contém a espécie de cada flor.\n","\n","O atributo `id_planta` é qualitativo, uma vez que é usado para identificar uma determinada amostra. Apesar dele possuir valores numéricos crescentes, ele exerce apenas a função de identificação e seus valores poderiam ser trocados por outros identificadores não numéricos sem nenhum prejuízo. O atributo `cidade_origem` também é qualitativo. Os atributos `comprimento_sepala`, `largura_sepala`, `comprimento_petala` e `largura_petala` são quantitativos contínuos.\n","\n","Quanto à escala, os atributos `id_planta` e `cidade_origem` são qualitativos nominais, enquanto os atributos `comprimento_sepala`, `largura_sepala`, `comprimento_petala` e `largura_petala` são quantitativos racionais.\n","\n","O atributo `classe` é qualitativo nominal e representa espécies de flores. Portanto, o problema em questão é de <b>aprendizado supervisionado</b> $\\rightarrow$ <b>classificação</b>.\n","\n","## Pré-processamento: eliminação de atributos irrelevantes\n","\n","O objetivo do problema é identificar a espécie de uma flor (`classe`), dados os demais atributos. Neste caso, não é preciso uma análise profunda para observar que os atributos `id_planta` e `cidade_origem` não contribuem para a identificação da classe. Portanto, em uma tarefa de aprendizado de máquina, devemos remover esses atributos, pois são irrelevantes. Em cenários reais, muitas vezes é necessário consultar especialistas para ajudar a identificar quais atributos são irrelevantes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SakDGKmYZ3_G"},"outputs":[],"source":["# remove as colunas id_planta e cidade_origem\n","df_dataset = df_dataset.drop(columns=['id_planta','cidade_origem'])\n","\n","# imprime o dataframe\n","display(df_dataset.head(n=10))"]},{"cell_type":"markdown","metadata":{"id":"1clQrdj_Fx6q"},"source":["## Pré-processamento: tratamento de atributos com valores ausentes\n","\n","Outro passo importante, é verificar se existem atributos com valores ausentes (*NaN*) na base de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQ4r6jX8Fx6r"},"outputs":[],"source":["# índices das linhas que contém valores NaN\n","idxRowNan = pd.isnull(df_dataset).any(1).to_numpy().nonzero()\n","\n","# imprime apenas as linhas com valoes ausentes\n","display(df_dataset.iloc[idxRowNan])"]},{"cell_type":"markdown","metadata":{"id":"AKUcElZ0Fx6r"},"source":["Como podemos ver, o atributo `largura_sepala` possui 2 amostras com valores ausentes. Já o atributo `comprimento_petala` possui 1 amostra com valor faltante. \n","\n","Existem diversas técnicas para tratar atributos faltantes. Como este problema possui poucos valores ausentes, vamos preencher esses valores com a média dos valores conhecidos da respectiva classe (`Iris-setosa`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTboUKUKFx6r"},"outputs":[],"source":["def trataFaltantes( df_dataset ):\n","    '''\n","    Substitui os valores faltantes pela média dos outros valores do mesmo atributo\n","    de amostras que sejam da mesma classe    \n","    '''\n","    \n","    # seleciona apenas as linhas da base de dados onde a coluna largura_sepala não contém valores nulos\n","    notNull_ls = df_dataset.loc[ ~pd.isnull(df_dataset['largura_sepala']), :]\n","    notNull_cp = df_dataset.loc[ ~pd.isnull(df_dataset['comprimento_petala']), :]\n","\n","    # calcula a media dos valores do atributo largura_sepala que não são nulos e que são da classe Iris-setosa \n","    media_ls = notNull_ls[ notNull_ls['classe']=='Iris-setosa' ]['largura_sepala'].mean()\n","    media_cp = notNull_cp[ notNull_cp['classe']=='Iris-setosa' ]['comprimento_petala'].mean()\n","\n","    # substitui os valores nulos pela média \n","    df_dataset.loc[ pd.isnull(df_dataset['largura_sepala']), 'largura_sepala'] = media_ls\n","    df_dataset.loc[ pd.isnull(df_dataset['comprimento_petala']), 'comprimento_petala'] = media_cp\n","    \n","    return df_dataset\n","\n","trataFaltantes( df_dataset )\n","    \n","# imprime apenas as linhas que antes possuiam valores NaN\n","print('\\nAmostras que possuiam valores faltantes:')\n","display(df_dataset.iloc[idxRowNan])"]},{"cell_type":"markdown","metadata":{"id":"EBTg1JLYFx6s"},"source":["## Pré-processamento: tratamento de dados inconsistentes ou redundantes\n","\n","Outro passo importante, é verificar se existem dados inconsistentes ou redundantes. A forma mais comum de inconsistência é quando há amostras representadas por atributos com todos os valores iguais, mas com classes diferentes. A redundância é dada pela repetição de linhas na base de dados.\n","\n","A seguir, vamos verificar se existem amostras duplicadas (redundantes) e inconsistentes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggSPrwNyFx6s"},"outputs":[],"source":["df_duplicates = df_dataset[ df_dataset.duplicated(subset=['comprimento_sepala','largura_sepala','comprimento_petala','largura_petala'],keep=False)] \n","\n","# se houver valores redundantes ou inconsistentes, imprima \n","if len(df_duplicates)>0:\n","    print('\\nAmostras redundantes ou inconsistentes:')\n","    display(df_duplicates)\n","else:\n","    print('Não existem valores duplicados')"]},{"cell_type":"markdown","metadata":{"id":"YPwquxSMFx6s"},"source":["Como podemos ver, existem algumas amostras redundantes (duplicadas) e outras inconsistentes (amostras iguais, mas com classes distintas). \n","\n","Primeiro, serão removidas as amostras redundantes, mantendo na base apenas uma delas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9U0pnZ9Fx6s","scrolled":true},"outputs":[],"source":["def delDuplicatas( df_dataset ):\n","    '''\n","    Para cada grupo de amostras duplicadas, mantém uma e apaga as demais\n","    '''\n","    \n","    # remove as amostras duplicadas, mantendo apenas a primeira ocorrencia\n","    df_dataset = df_dataset.drop_duplicates(keep = 'first')    \n","\n","    return df_dataset\n","\n","df_dataset = delDuplicatas( df_dataset )\n"]},{"cell_type":"markdown","metadata":{"id":"eEqeYdKhFx6t"},"source":["Após remover as amostras redundantes, é preciso checar se há amostras inconsistentes. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uv6_RRKUFx6t"},"outputs":[],"source":["# para detectar inconsistências, a rotina abaixo obtém as amostras onde os valores \n","# dos atributos continuam duplicados. Neste caso, os atributos serão iguais, mas as classes serão distintas\n","df_duplicates = df_dataset[ df_dataset.duplicated(subset=['comprimento_sepala','largura_sepala','comprimento_petala','largura_petala'],keep=False)] \n","\n","# se tiver valores inconsistentes, imprime \n","if len(df_duplicates)>0:\n","    print('\\nAmostras inconsistentes:')\n","    display(df_duplicates)\n","else:\n","    print('Não existem mostras inconsistentes')\n","    "]},{"cell_type":"markdown","metadata":{"id":"j_D8VY_iFx6t"},"source":["Podemos ver que existem duas amostras inconsistentes. Nesse caso, como não é possível saber qual delas está correta, as duas serão eliminadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U06CNCckFx6t"},"outputs":[],"source":["def delInconsistencias( df_dataset ):\n","    '''\n","    Remove todas as amostras inconsistentes da base de dados\n","    '''\n","\n","    df_dataset = df_dataset.drop_duplicates(subset=['comprimento_sepala','largura_sepala','comprimento_petala','largura_petala'], keep = False)    \n","  \n","    return df_dataset\n","\n","df_dataset = delInconsistencias( df_dataset )\n","\n","# obtém apenas as amostras onde os valores dos atributos estão duplicados\n","df_duplicates = df_dataset[ df_dataset.duplicated(subset=['comprimento_sepala','largura_sepala','comprimento_petala','largura_petala'],keep=False)] \n","\n","# se tiver valores redundantes ou inconsistentes, imprime \n","if len(df_duplicates)>0:\n","    display(df_duplicates)\n","else:\n","    print('Não existem amostras redundantes ou inconsistentes')\n","    "]},{"cell_type":"markdown","metadata":{"id":"-eRw4-sbFx6u"},"source":["Agora, vamos gerar algumas estatísticas sobre a base de dados.\n","\n","A função `describe()` da `Pandas` sumariza as principais estatísticas sobre os dados de um *data frame*, como a média, o desvio padrão, valor máximo, valor mínimo e alguns percentis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nj82p9gpFx6u"},"outputs":[],"source":["# apresenta as principais estatísticas da base de dados\n","df_detalhes = df_dataset.describe()\n","\n","display(df_detalhes)"]},{"cell_type":"markdown","metadata":{"id":"6XsbH645Fx6u"},"source":["## Pré-processamento: normalização dos atributos\n","\n","Observe que a média do atributo `comprimento_sepala` é bastante superior a média do atributo `largura_petala`. Diante disso, está claro que a escala dos atributos é diferente, o que pode prejudicar alguns métodos de aprendizado de máquina. Portanto, vamos normalizar os valores dos atributos para que fiquem com média igual a zero e desvio padrão igual a um. Usando a biblioteca *scikit-learn*, poderíamos fazer a normalização usando a função [*sklearn.preprocessing.StandardScaler*](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Mas, para exercitar os conceitos aprendidos, vamos criar nossa própria função de normalização. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZ-PR5gRFx6u"},"outputs":[],"source":["def normalizar(X):\n","    \"\"\"\n","    Normaliza os atributos em X\n","    \n","    Esta função retorna uma versao normalizada de X onde o valor da\n","    média de cada atributo é igual a 0 e desvio padrao é igual a 1. Trata-se de\n","    um importante passo de pré-processamento quando trabalha-se com \n","    métodos de aprendizado de máquina.\n","    \"\"\"\n","    \n","    m, n = X.shape # m = qtde de objetos e n = qtde de atributos por objeto\n","    \n","    # Incializa as variaves de saída\n","    X_norm = np.random.rand(m,n) # inicializa X_norm com valores aleatórios\n","    mu = 0 # inicializa a média\n","    sigma = 1 # inicializa o desvio padrão\n","     \n","    mu = np.mean(X, axis=0)\n","    sigma = np.std(X, axis=0, ddof=1)\n","    \n","    for i in range(m):\n","        X_norm[i,:] = (X[i,:]-mu) / sigma\n","        \n","    \n","    return X_norm, mu, sigma\n","\n","\n","# coloca os valores dos atributos na variável X\n","X = df_dataset.iloc[:,0:-1].values\n","\n","# chama a função para normalizar X\n","X_norm, mu, sigma = normalizar(X)\n","\n","df_dataset.iloc[:,0:-1] = X_norm\n","\n","print('\\nPrimeira amostra da base antes da normalização: [%2.4f %2.4f].' %(X[0,0],X[0,1]))\n","print('\\nApós a normalização, espera-se que a primeira amostra seja igual a: [-0.5747 0.1804].')\n","print('\\nPrimeira amostra da base apos normalização: [%2.4f %2.4f].' %(X_norm[0,0],X_norm[0,1]))\n"]},{"cell_type":"markdown","metadata":{"id":"xDqkG6BuFx6v"},"source":["Agora que os dados estão normalizados, vamos analisar as informações estatísticas novamente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBwEx5azFx6v"},"outputs":[],"source":["# apresenta as principais estatísticas da base de dados\n","df_detalhes = df_dataset.describe()\n","\n","display(df_detalhes.round(8))"]},{"cell_type":"markdown","metadata":{"id":"a8J3n-QoFx6v"},"source":["Podemos ver acima que a média (*mean*) ficou igual a 0 e o desvio padrão (*std*) igual a 1. \n","\n","## Pré-processamento: detecção de *outliers*\n","\n","Outro passo importante na análise e tratamento dos dados é a detecção de *outliers* (*i.e.*, dados gerados por leituras incorretas, erros de digitação, etc). \n","\n","Uma das maneiras mais simples de verificar se os dados contém *outliers* é criar um gráfico box plot de cada atributo. Para isso, podemos usar a função `boxplot` da biblioteca `Pandas`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rnx7viJwFx6v"},"outputs":[],"source":["# gera um bloxplot para cada atributo\n","df_dataset.boxplot(figsize=(15,7))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4kimXaX0Fx6v"},"source":["O box plot está indicando que os atributos `comprimento_sepala` e `largura_sepala` possuem *outliers*, o que pode prejudicar o desempenho de vários métodos de aprendizado de máquina, pois tratam-se de amostras com valores de atributos incorretos. \n","\n","Outra forma de analisar se a base de dados contém *outliers* é usar gráficos de dispersão. Podemos plotar gráficos de dispersão de todas as combinações de atributos da base de dados usando a função `scatter_matrix` da `Pandas`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2b1gfF9Fx6w"},"outputs":[],"source":["pd.plotting.scatter_matrix(df_dataset, figsize=(18,18))\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eTDpGJafFx6w"},"source":["Outra forma de plotar gráficos de dispersão a partir dos _dataframes_ é usando a biblioteca `Seaborn`. Juntamente com essa biblioteca, também é recomendável importar a biblioteca `Matplotlib` para personalizar os gráficos. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sh0Fk1kPFx6w","scrolled":true},"outputs":[],"source":["# matriz de gráficos scatter \n","sns.pairplot(df_dataset, hue='classe', height=3.5);\n","\n","# mostra o gráfico usando a função show() da matplotlib\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"qo3zrfxCFx6w"},"source":["Observando os gráficos de dispersão, é fácil perceber que existem duas amostras da classe *Iris-virginica* que estão deslocadas no espaço em relação às demais amostras.\n","\n","Pelos gráficos, os *outliers* parecem ser mais visíveis na combinação dos atributos `comprimento_sepala` e `largura_sepala`. Então, vamos usar a função `lmplot` da biblioteca `Seaborn`para visualizar a combinação desses dois atributos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GR2EjlUFx6w"},"outputs":[],"source":["# define o scatter plot\n","sns.lmplot(x='comprimento_sepala', y='largura_sepala', data=df_dataset, \n","           fit_reg=False,  \n","           hue='classe')\n","\n","# cria um título para o gráfico\n","plt.title('Comprimento vs largura da sepala')\n","\n","# mostra o gráfico\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jm0Ozj4UFx6x"},"source":["Pelos gráficos vistos até o momento, fica claro que um dos *outliers* possui um alto valor no atributo `largura_sepala`. Já o segundo outlier contém um alto valor no atributo `comprimento_sepala`. \n","\n","A bilioteca `Seaborn` permite criar gráficos boxplot agrupados por um determinado atributo, o que facilita a análise dos dados. No exemplo abaixo, criaremos boxplots para cada atributo agrupados pela classe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SI4heRb5Fx6x"},"outputs":[],"source":["for atributo in df_dataset.columns[:-1]:\n","    # define a dimensão do gráfico\n","    plt.figure(figsize=(8,8))\n","\n","    # cria o boxplot\n","    sns.boxplot(x=\"classe\", y=atributo, data=df_dataset, whis=1.5)\n","\n","    # mostra o gráfico\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NHGG2Um_Fx6x"},"source":["Os box plots dos atributos mostraram outros *outliers* que não haviam aparecido no primeiro box plot. Portanto, esses novos valores são considerados *outliers* se analisarmos as classes individualmente, mas não são considerados *outliers* se analisarmos a base de dados de forma geral. \n","\n","Outro tipo de gráfico que ajuda a detectar *outliers* é o histograma. Portanto, vamos usá-lo para analisar cada atributo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41DQgDiqFx6x"},"outputs":[],"source":["for atributo in df_dataset.columns[:-1]:\n","    \n","    # cria o histograma\n","    n, bins, patches = plt.hist(df_dataset[atributo].values,bins=10, color='red', edgecolor='black', linewidth=0.9)\n","\n","    # cria um título para o gráfico\n","    plt.title(atributo)\n","\n","    # mostra o gráfico\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tUvW_3gFFx6x"},"source":["Nos histogramas, os *outliers* mais evidentes estão nos atributos `comprimento_sepala` e `largura_sepala`.\n","\n","Agora, vamos usar um gráfico de densidade para fazer o mesmo tipo de análise."]},{"cell_type":"markdown","metadata":{"id":"qNmaTIDEFx6y"},"source":["\n","Uma das maneiras mais simples de tratar *outliers* é remover aqueles valores que são menores que $Q1 - 1.5 * IQR$ ou maiores que $Q3 + 1.5 * IQR$, onde $Q1$ é o primeiro quartil, $Q3$ é o terceiro quartil e $IQR$ é o intervalo interquartil. O IQR pode ser calculado pela seguinte equação: $IQR = Q3-Q1$. \n","\n","Com base nessas informações, vamos usar a função abaixo para remover os *outliers* da base de dados. Usaremos como base o IQR de cada atributo em relação a todos os valores na base de dados, em vez do IQR individual de cada classe."]},{"cell_type":"markdown","metadata":{"id":"sT7x7fWuoTSO"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeAmEopqFx6y"},"outputs":[],"source":["def removeOutliers(df_dataset):\n","    \"\"\"\n","    Remove os outliers da base de dados \n","    \"\"\"\n","    \n","    for atributo in df_dataset.columns[:-1]:\n","\n","        # obtem o terceiro e o primeiro quartil. \n","        q75, q25 = np.percentile(df_dataset[atributo].values, [75 ,25])\n","        \n","        # calcula o IQR\n","        IQR = q75 - q25\n","\n","        # remove os outliers com base no valor do IQR\n","        df_dataset = df_dataset[ (df_dataset[atributo]<=(q75+1.5*IQR)) & (df_dataset[atributo]>=(q25-1.5*IQR)) ]\n","    \n","    return df_dataset\n","\n","# remove os outliers\n","df_dataset = removeOutliers( df_dataset )\n","\n","# apresenta as principais estatísticas sobre a base de dados\n","df_dataset.boxplot(figsize=(15,7))\n","plt.show()\n","\n","# matriz de gráficos scatter \n","sns.pairplot(df_dataset, hue='classe', height=3.5);\n","\n","# mostra o gráfico usando a função show() da matplotlib\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sCzkybYzFx6y"},"source":["Depois da remoção, o box plot e os gráficos de dispersão indicam que não há mais nenhum *outlier* na base de dados. \n","\n","Com os novos gráficos de dispersão, também é possível perceber que a classe *Iris-setosa* é mais fácil de identificar, pois está mais separada no espaço de atributos. Por outro lado, em várias combinações de atributos, as classes *Iris-versicolor* e *Iris-virginica* se misturam.\n","\n","**IMPORTANTE:** antes de realizar a remoção de *outliers*, é mandatório analisar cuidadosamente as características das amostras antes de removê-las. Em alguns casos, remover os *outliers* pode ser prejudicial. Além disso, algumas tarefas de aprendizado de máquina são voltadas para a detecção de *outliers* e, portanto, esses dados não podem ser removidos. Adicionalmente, se a base de dados for desbalanceada, a remoção dos *outliers* com base nas estatísticas de toda a base, pode acabar removendo amostras da classe minoritária (aquela que possui menos amostras). Ainda, alguns métodos de classificação, tais como métodos baseados em *ensemble* e métodos baseados em árvores, costumam ser robustos a *outliers*. Diante disso, em alguns problemas, é recomendável remover apenas aqueles *outliers* que são claramente erros de leitura/digitação, isto é, valores que estão fora dos limites aceitáveis para o que é esperado para um determinado atributo (por exemplo, uma pessoa com 500 anos ou um bebê com 300 kg). \n","\n","## Pré-processamento: distribuição das classes\n","\n","Outro passo importante na análise de dados é verificar a distribuição das classes. Para isso, é possível criar um gráfico de barra indicando quantas amostras de cada classe há na base de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6c8tfyaOFx6y"},"outputs":[],"source":["display( df_dataset['classe'].value_counts() )\n","\n","# cria um gráfico de barras com a frequência de cada classe\n","sns.countplot(x=\"classe\", data=df_dataset)\n","\n","# mostra o gráfico\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"l-oYynSyFx6y"},"source":["Conforme podemos ver, as classes são balanceadas. Se o número de exemplos em alguma das classes fosse muito superior às demais, teríamos que usar alguma técnica de balanceamento de classes, pois o modelo gerado pela maioira dos métodos de aprendizado supervisionado costuma ser tendencioso para as classes com maior número de amostras. "]},{"cell_type":"markdown","metadata":{"id":"QvWxXqV9Fx6y"},"source":["## Pré-processamento: correlação entre os atributos\n","\n","Quando dois atributos possuem valores idênticos ou muito semelhantes para todas as amostras, um deles deve ser eliminado ou eles devem ser combinados. Isso ajuda a diminuir o custo computacional das tarefas de aprendizado e evita que o aprendizado de alguns método seja prejudicado, principalmente os métodos baseados em otimização.\n","\n","Uma das maneiras mais comuns de analisar a correlação dos dados é através das matrizes de correlação e covariância. Podemos fazer isso usando a biblioteca `Numpy` ou a `Pandas`.\n","\n","Primeiro, vamos fazer usando a `Numpy`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBWTavaDFx6z"},"outputs":[],"source":["# criando uma matriz X com os valores do data frame\n","X = df_dataset.iloc[:,:-1].values\n","\n","# matriz de covariancia\n","covariance = np.cov(X, rowvar=False)\n","\n","# matriz de correlação\n","correlation = np.corrcoef(X, rowvar=False)\n","\n","print('Matriz de covariância: ')\n","display(covariance)\n","\n","print('\\n\\nMatriz de correlação: ')\n","display(correlation)"]},{"cell_type":"markdown","metadata":{"id":"VJH_bKTDFx6z"},"source":["Agora, vamos calcular as matrizes de correlação e covariância usando a `Pandas`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28KbQY3RFx6z"},"outputs":[],"source":["# matriz de covariancia\n","df_covariance = df_dataset.cov()\n","\n","# matriz de correlação\n","df_correlation = df_dataset.corr()\n","\n","print('Matriz de covariância: ')\n","display(df_covariance)\n","\n","print('\\n\\nMatriz de correlação: ')\n","display(df_correlation)"]},{"cell_type":"markdown","metadata":{"id":"BSAKvYNGFx6z"},"source":["Podemos ver que os atributos `comprimento_petala` e `largura_petala` possuem alta covariância e alta correlação. Se o problema que estamos analisando tivesse muitos atributos, poderíamos pensar na possibilidade de combinar esses dois atributos. Se a correlação entre dois atributos for igual a 1 ou -1, significa que eles são redundantes e um deles poderia ser eliminado.\n","\n","Para facilitar a visualização, vamos plotar a matriz de covariância e a de correlação usando mapas de cores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFVScuIEFx6z"},"outputs":[],"source":["# cria um mapa de cores dos valores da covariancia\n","sns.heatmap(df_covariance, \n","        xticklabels=df_correlation.columns,\n","        yticklabels=df_correlation.columns)\n","\n","plt.title('Covariancia')\n","plt.show()\n","\n","# cria um mapa de cores dos valores da correlação\n","sns.heatmap(df_correlation, \n","        xticklabels=df_correlation.columns,\n","        yticklabels=df_correlation.columns)\n","\n","plt.title('Correlacao')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"h_e5P56CFx60"},"source":["---\n","## Conclusão\n","\n","Neste notebook, foram mostradas as principais etapas de visualização, interpretação e pré-processamento dos dados. Foi apresentado como  eliminar atributos  irrelevantes e tratar dados faltantes, redundantes ou inconsistentes. Além disso, foi mostrado como deve ser feita a normalização dos dados e quais os possíveis impactos dessa etapa no desempenho dos métodos de aprendizado. Ainda, foi mostrada uma das técnicas de remoção de outliers, como visualizar a distribuição das classes e como analisar a correlação dos atributos. Para obter maiores detalhes teóricos sobre os conceitos apresentados, consulte os Capítulos 2 (Análise de Dados) e 3 (Pré-processamento de Dados)."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"notebook_analise_e_pre_processamento_de_dados.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
